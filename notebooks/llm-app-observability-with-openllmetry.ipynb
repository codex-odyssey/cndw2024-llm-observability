{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OpenLLMetryによるLLMアプリケーションの計装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "必要なライブラリをダウンロードする。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vector Storeの初期化を行う"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, logging\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_cohere import CohereEmbeddings\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "import faiss\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "_ = load_dotenv(find_dotenv())\n",
    "openai_api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "cohere_api_key = os.getenv(\"COHERE_API_KEY\")\n",
    "\n",
    "model_provider_name = \"cohere\" # or openai\n",
    "\n",
    "if model_provider_name == \"openai\":\n",
    "    embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "elif model_provider_name == \"cohere\":\n",
    "    embeddings = CohereEmbeddings(\n",
    "        cohere_api_key=cohere_api_key, model=\"embed-multilingual-v3.0\"\n",
    ")\n",
    "else:\n",
    "    logger.error(\"Unsetted model name\")\n",
    "\n",
    "embeddings = OpenAIEmbeddings(api_key=openai_api_key)\n",
    "index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "vector_store = FAISS(\n",
    "    embedding_function=embeddings,\n",
    "    index=index,\n",
    "    docstore=InMemoryDocstore(),\n",
    "    index_to_docstore_id={},\n",
    ")\n",
    "\n",
    "documents = CSVLoader(file_path=\"../data/cndw2024_accepted_sessions.csv\").load()\n",
    "\n",
    "result = vector_store.add_documents(documents=documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "計装対象のアプリケーションを実装する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_cohere import ChatCohere\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from traceloop.sdk import Traceloop\n",
    "\n",
    "# OTLP/GRPC\n",
    "# os.environ[\"TRACELOOP_BASE_URL\"] = \"localhost:4317\"\n",
    "# OTLP/HTTP\n",
    "os.environ[\"TRACELOOP_BASE_URL\"] = \"http://localhost:4318\"\n",
    "\n",
    "Traceloop.init(\n",
    "    # デモ用なので、batch processorではなく即時でトレースデータを送る\n",
    "    disable_batch=True,\n",
    "    # アプリケーションの名前\n",
    "    app_name=\"Ask the BigBaBy\",\n",
    "    # 独自属性の追加\n",
    "    resource_attributes={\n",
    "        \"env\": \"demo\",\n",
    "        \"version\": \"1.0.0\"\n",
    "    },\n",
    ")\n",
    "model_provider_name = \"cohere\" # or openai\n",
    "temperature = 1.0\n",
    "max_tokens = 2024\n",
    "\n",
    "if model_provider_name == \"openai\":\n",
    "    chat_model = ChatOpenAI(\n",
    "        api_key=openai_api_key,\n",
    "        model=\"gpt-4o-mini\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "elif model_provider_name == \"cohere\":\n",
    "    chat_model = ChatCohere(\n",
    "        cohere_api_key=cohere_api_key,\n",
    "        model=\"command-r-plus\",\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "else:\n",
    "    logger.error(\"Unsetted model name\")\n",
    "\n",
    "prompt = PromptTemplate.from_template(template=\"\"\"\n",
    "以下の質問をコンテキストに基づいて、答えてください。\n",
    "\n",
    "## コンテキスト\n",
    "{context}\n",
    "\n",
    "## 質問\n",
    "{question}\n",
    "\"\"\")\n",
    "\n",
    "chain = (\n",
    "    {\"question\": RunnablePassthrough(), \"context\": vector_store.as_retriever()}\n",
    "    | prompt\n",
    "    | chat_model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "response = chain.invoke(input=\"Kubernetes関係でおすすめのセッションはありますか？\")\n",
    "\n",
    "print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
